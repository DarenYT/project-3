{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4d1c86",
   "metadata": {},
   "source": [
    "# Project 3 Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "610c4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import time\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f738279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bffb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'before' parameter set as 0000hr 00min 00sec on the 18th March so as to recent enough for the posts \n",
    "# to still be topical but not with a gap of ~ a week for submissions to be moderated\n",
    "params = {\n",
    "    'subreddit': 'depression',\n",
    "    'size': 100,\n",
    "    'before': 1647561600\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09cdbbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url,  params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d53e9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "980943d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.width = 1200\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d202997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94276164",
   "metadata": {},
   "source": [
    "## Depression loop to get posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "53a159ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 posts gathered.\n",
      "145 posts gathered.\n",
      "218 posts gathered.\n",
      "288 posts gathered.\n",
      "352 posts gathered.\n",
      "409 posts gathered.\n",
      "475 posts gathered.\n",
      "540 posts gathered.\n",
      "612 posts gathered.\n",
      "680 posts gathered.\n",
      "745 posts gathered.\n",
      "809 posts gathered.\n",
      "874 posts gathered.\n",
      "936 posts gathered.\n",
      "1002 posts gathered.\n",
      "1063 posts gathered.\n",
      "1127 posts gathered.\n",
      "1195 posts gathered.\n",
      "1263 posts gathered.\n",
      "1329 posts gathered.\n",
      "1395 posts gathered.\n",
      "1465 posts gathered.\n",
      "1529 posts gathered.\n",
      "1594 posts gathered.\n",
      "1666 posts gathered.\n",
      "1741 posts gathered.\n",
      "1804 posts gathered.\n",
      "1862 posts gathered.\n",
      "1933 posts gathered.\n",
      "1988 posts gathered.\n",
      "2043 posts gathered.\n",
      "2110 posts gathered.\n",
      "2180 posts gathered.\n",
      "2247 posts gathered.\n",
      "2324 posts gathered.\n",
      "2389 posts gathered.\n",
      "2459 posts gathered.\n",
      "2531 posts gathered.\n",
      "2591 posts gathered.\n",
      "2659 posts gathered.\n",
      "2730 posts gathered.\n",
      "2802 posts gathered.\n",
      "2868 posts gathered.\n",
      "2936 posts gathered.\n",
      "2999 posts gathered.\n",
      "3069 posts gathered.\n",
      "3135 posts gathered.\n",
      "3205 posts gathered.\n",
      "3270 posts gathered.\n",
      "3340 posts gathered.\n",
      "3408 posts gathered.\n",
      "3469 posts gathered.\n",
      "3540 posts gathered.\n",
      "3601 posts gathered.\n",
      "3665 posts gathered.\n",
      "3733 posts gathered.\n",
      "3796 posts gathered.\n",
      "3867 posts gathered.\n",
      "3936 posts gathered.\n",
      "4000 posts gathered.\n",
      "4066 posts gathered.\n",
      "4133 posts gathered.\n",
      "4194 posts gathered.\n",
      "4266 posts gathered.\n",
      "4335 posts gathered.\n",
      "4403 posts gathered.\n",
      "4475 posts gathered.\n",
      "4551 posts gathered.\n",
      "4621 posts gathered.\n",
      "4685 posts gathered.\n",
      "4754 posts gathered.\n",
      "4826 posts gathered.\n",
      "4893 posts gathered.\n",
      "4960 posts gathered.\n",
      "5024 posts gathered.\n",
      "5098 posts gathered.\n",
      "5167 posts gathered.\n",
      "5233 posts gathered.\n",
      "5299 posts gathered.\n",
      "5351 posts gathered.\n",
      "5423 posts gathered.\n",
      "5493 posts gathered.\n",
      "5552 posts gathered.\n",
      "5610 posts gathered.\n",
      "5678 posts gathered.\n",
      "5749 posts gathered.\n",
      "5810 posts gathered.\n",
      "5877 posts gathered.\n",
      "5943 posts gathered.\n",
      "6009 posts gathered.\n",
      "6079 posts gathered.\n",
      "6146 posts gathered.\n",
      "6211 posts gathered.\n",
      "6286 posts gathered.\n",
      "6346 posts gathered.\n",
      "6405 posts gathered.\n",
      "6465 posts gathered.\n",
      "6539 posts gathered.\n",
      "6613 posts gathered.\n",
      "6686 posts gathered.\n",
      "6756 posts gathered.\n",
      "6825 posts gathered.\n",
      "6890 posts gathered.\n",
      "6964 posts gathered.\n",
      "7041 posts gathered.\n",
      "7112 posts gathered.\n",
      "7176 posts gathered.\n",
      "7242 posts gathered.\n",
      "7315 posts gathered.\n",
      "7386 posts gathered.\n",
      "7456 posts gathered.\n",
      "7520 posts gathered.\n",
      "7587 posts gathered.\n",
      "7662 posts gathered.\n",
      "7731 posts gathered.\n",
      "7803 posts gathered.\n",
      "7871 posts gathered.\n",
      "7941 posts gathered.\n",
      "8018 posts gathered.\n",
      "8083 posts gathered.\n",
      "8151 posts gathered.\n",
      "8219 posts gathered.\n",
      "8292 posts gathered.\n",
      "8358 posts gathered.\n",
      "8428 posts gathered.\n",
      "8492 posts gathered.\n",
      "8562 posts gathered.\n",
      "8633 posts gathered.\n",
      "8710 posts gathered.\n",
      "8781 posts gathered.\n",
      "8857 posts gathered.\n",
      "8932 posts gathered.\n",
      "9014 posts gathered.\n",
      "9085 posts gathered.\n",
      "9153 posts gathered.\n",
      "9213 posts gathered.\n",
      "9283 posts gathered.\n",
      "9349 posts gathered.\n",
      "9405 posts gathered.\n",
      "9475 posts gathered.\n",
      "9541 posts gathered.\n",
      "9607 posts gathered.\n",
      "9673 posts gathered.\n",
      "9735 posts gathered.\n",
      "9798 posts gathered.\n",
      "9865 posts gathered.\n",
      "9933 posts gathered.\n",
      "10018 posts gathered.\n",
      "CPU times: total: 7.94 s\n",
      "Wall time: 30min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### ~31 MINUTES RUNTIME! ####\n",
    "\n",
    "data_list = []\n",
    "while len(data_list) < 10000:\n",
    "    res = requests.get(url,  params)\n",
    "    data = res.json()\n",
    "    for _ in range(len(data['data'])):\n",
    "        try:\n",
    "            \"\"\"\n",
    "            filtering data to reduce cleaning required in later stages\n",
    "            - avoiding scraping removed/deleted posts\n",
    "            - using length of tokens to ensure posts have some degree of substance         \n",
    "            \"\"\"\n",
    "            if data['data'][_]['title'] != '[removed]' and \\\n",
    "               data['data'][_]['title'] != '[deleted]' and \\\n",
    "               len(tokenizer.tokenize(data['data'][_]['title'])) > 0 and \\\n",
    "               len(tokenizer.tokenize(data['data'][_]['title'])) < 16 and \\\n",
    "               data['data'][_]['selftext'] != '[removed]' and \\\n",
    "               data['data'][_]['selftext'] != '[deleted]' and \\\n",
    "               len(tokenizer.tokenize(data['data'][_]['selftext'])) > 5 and \\\n",
    "               len(tokenizer.tokenize(data['data'][_]['selftext'])) < 401:\n",
    "\n",
    "                data_list.append((\n",
    "                    data['data'][_]['title'].lower(), \n",
    "                    data['data'][_]['selftext'].lower()\n",
    "                ))\n",
    "            else: \n",
    "                pass\n",
    "            \"\"\"\n",
    "            In the event of a KeyError, ValueError ,or ChunkedEncodingError due to a break in the data streaming \n",
    "            of the API, the loop will find the last post it processed correctly and restart from there\n",
    "            \"\"\"\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except ValueError:\n",
    "            pass\n",
    "        except ChunkedEncodingError:\n",
    "            pass\n",
    "            \n",
    "    params = {\n",
    "            'subreddit': 'depression',\n",
    "            'size': 100,\n",
    "            'before': data['data'][-1]['created_utc']\n",
    "            }\n",
    "    time.sleep(5)\n",
    "    print(f\"{len(data_list)} posts gathered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82c58012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of depression posts\n",
    "dep_count = len(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836878d",
   "metadata": {},
   "source": [
    "### Saving Scraped r/Depression Data as a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68d0946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for depression posts\n",
    "depression = pd.DataFrame(data_list[:dep_count], columns=['title', 'selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3cde62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "depression.to_csv('depression.csv', index_label = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402601d3",
   "metadata": {},
   "source": [
    "## r/SuicideWatch loop to get posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "246b03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RUN THIS CODE IF R/SUICIDEWATCH SCRAPING BELOW HAS A CONNECTION ISSUE AND YOU NEED TO REASSIGN\n",
    "#### dep_count WITHOUT RUNNING THE r/Depression DATA SCRAPING LOOP\n",
    "\n",
    "depression = pd.read_csv('depression.csv')\n",
    "data_list = [(x, y) for x, y in zip(depression['title'], depression['selftext'])]\n",
    "dep_count = len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3407369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10084 posts gathered.\n",
      "10133 posts gathered.\n",
      "10188 posts gathered.\n",
      "10257 posts gathered.\n",
      "10325 posts gathered.\n",
      "10393 posts gathered.\n",
      "10457 posts gathered.\n",
      "10528 posts gathered.\n",
      "10597 posts gathered.\n",
      "10669 posts gathered.\n",
      "10740 posts gathered.\n",
      "10803 posts gathered.\n",
      "10870 posts gathered.\n",
      "10935 posts gathered.\n",
      "11005 posts gathered.\n",
      "11063 posts gathered.\n",
      "11134 posts gathered.\n",
      "11203 posts gathered.\n",
      "11266 posts gathered.\n",
      "11335 posts gathered.\n",
      "11400 posts gathered.\n",
      "11472 posts gathered.\n",
      "11539 posts gathered.\n",
      "11607 posts gathered.\n",
      "11671 posts gathered.\n",
      "11737 posts gathered.\n",
      "11793 posts gathered.\n",
      "11858 posts gathered.\n",
      "11919 posts gathered.\n",
      "11992 posts gathered.\n",
      "12064 posts gathered.\n",
      "12130 posts gathered.\n",
      "12186 posts gathered.\n",
      "12255 posts gathered.\n",
      "12319 posts gathered.\n",
      "12388 posts gathered.\n",
      "12464 posts gathered.\n",
      "12525 posts gathered.\n",
      "12594 posts gathered.\n",
      "12655 posts gathered.\n",
      "12724 posts gathered.\n",
      "12795 posts gathered.\n",
      "12858 posts gathered.\n",
      "12934 posts gathered.\n",
      "12993 posts gathered.\n",
      "13059 posts gathered.\n",
      "13123 posts gathered.\n",
      "13189 posts gathered.\n",
      "13260 posts gathered.\n",
      "13336 posts gathered.\n",
      "13403 posts gathered.\n",
      "13468 posts gathered.\n",
      "13537 posts gathered.\n",
      "13605 posts gathered.\n",
      "13669 posts gathered.\n",
      "13737 posts gathered.\n",
      "13806 posts gathered.\n",
      "13867 posts gathered.\n",
      "13936 posts gathered.\n",
      "13990 posts gathered.\n",
      "14050 posts gathered.\n",
      "14109 posts gathered.\n",
      "14178 posts gathered.\n",
      "14238 posts gathered.\n",
      "14302 posts gathered.\n",
      "14361 posts gathered.\n",
      "14433 posts gathered.\n",
      "14502 posts gathered.\n",
      "14567 posts gathered.\n",
      "14634 posts gathered.\n",
      "14690 posts gathered.\n",
      "14753 posts gathered.\n",
      "14815 posts gathered.\n",
      "14885 posts gathered.\n",
      "14951 posts gathered.\n",
      "15016 posts gathered.\n",
      "15084 posts gathered.\n",
      "15154 posts gathered.\n",
      "15214 posts gathered.\n",
      "15283 posts gathered.\n",
      "15352 posts gathered.\n",
      "15424 posts gathered.\n",
      "15493 posts gathered.\n",
      "15569 posts gathered.\n",
      "15629 posts gathered.\n",
      "15703 posts gathered.\n",
      "15774 posts gathered.\n",
      "15849 posts gathered.\n",
      "15915 posts gathered.\n",
      "15982 posts gathered.\n",
      "16048 posts gathered.\n",
      "16114 posts gathered.\n",
      "16184 posts gathered.\n",
      "16255 posts gathered.\n",
      "16324 posts gathered.\n",
      "16396 posts gathered.\n",
      "16471 posts gathered.\n",
      "16543 posts gathered.\n",
      "16620 posts gathered.\n",
      "16693 posts gathered.\n",
      "16761 posts gathered.\n",
      "16828 posts gathered.\n",
      "16901 posts gathered.\n",
      "16967 posts gathered.\n",
      "17043 posts gathered.\n",
      "17120 posts gathered.\n",
      "17182 posts gathered.\n",
      "17252 posts gathered.\n",
      "17318 posts gathered.\n",
      "17393 posts gathered.\n",
      "17462 posts gathered.\n",
      "17524 posts gathered.\n",
      "17595 posts gathered.\n",
      "17665 posts gathered.\n",
      "17732 posts gathered.\n",
      "17796 posts gathered.\n",
      "17861 posts gathered.\n",
      "17937 posts gathered.\n",
      "18006 posts gathered.\n",
      "18074 posts gathered.\n",
      "18135 posts gathered.\n",
      "18216 posts gathered.\n",
      "18281 posts gathered.\n",
      "18354 posts gathered.\n",
      "18418 posts gathered.\n",
      "18493 posts gathered.\n",
      "18557 posts gathered.\n",
      "18623 posts gathered.\n",
      "18689 posts gathered.\n",
      "18756 posts gathered.\n",
      "18822 posts gathered.\n",
      "18888 posts gathered.\n",
      "18948 posts gathered.\n",
      "19006 posts gathered.\n",
      "19080 posts gathered.\n",
      "19146 posts gathered.\n",
      "19216 posts gathered.\n",
      "19281 posts gathered.\n",
      "19341 posts gathered.\n",
      "19405 posts gathered.\n",
      "19477 posts gathered.\n",
      "19536 posts gathered.\n",
      "19599 posts gathered.\n",
      "19668 posts gathered.\n",
      "19736 posts gathered.\n",
      "19798 posts gathered.\n",
      "19855 posts gathered.\n",
      "19918 posts gathered.\n",
      "19983 posts gathered.\n",
      "20036 posts gathered.\n",
      "CPU times: total: 7.97 s\n",
      "Wall time: 33min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### ~33 MINUTES RUNTIME! ####\n",
    "\n",
    "# 'before' parameter set as 0000hr 00min 00sec on the 18th March so as to recent enough for the posts \n",
    "# to still be topical but not with a gap of ~ a week for submissions to be moderated\n",
    "params = {\n",
    "         'subreddit': 'suicidewatch',\n",
    "         'size': 100,\n",
    "         'before': 1647561600 \n",
    "         }\n",
    "\n",
    "# Loop until 10000\n",
    "\n",
    "while len(data_list) < 2 * dep_count:\n",
    "    res = requests.get(url,  params)\n",
    "    data = res.json()\n",
    "    for _ in range(len(data['data'])):\n",
    "        try:\n",
    "            \"\"\"\n",
    "            filtering data to reduce cleaning required in later stages\n",
    "            - avoiding scraping removed/deleted posts\n",
    "            - using length of tokens to ensure posts have some degree of substance\n",
    "                - the minimum and maximum values for the title tokens and selftext tokens were chosen \n",
    "                  based on previous scraping efforts\n",
    "                - histograms of previous sscraping efforts indicated that the majority of posts were within these \n",
    "                  parameters\n",
    "                - while we would endeavour to reduce bias in our models by ignoring outliers, we still have to \n",
    "                  treat every data point as credible even though they might not be accepted were we dealing with \n",
    "                  a different subreddit i.e. posts with single-word titles\n",
    "            \"\"\"\n",
    "            if data['data'][_]['title'] != '[removed]' and \\\n",
    "               data['data'][_]['title'] != '[deleted]' and \\\n",
    "               len(tokenizer.tokenize(data['data'][_]['title'])) > 0 and \\\n",
    "               len(tokenizer.tokenize(data['data'][_]['title'])) < 16 and \\\n",
    "               data['data'][_]['selftext'] != '[removed]' and \\\n",
    "               data['data'][_]['selftext'] != '[deleted]' and \\\n",
    "               len(tokenizer.tokenize(data['data'][_]['selftext'])) > 5 and \\\n",
    "               len(tokenizer.tokenize(data['data'][_]['selftext'])) < 401:\n",
    "\n",
    "                data_list.append((\n",
    "                    data['data'][_]['title'].lower(), \n",
    "                    data['data'][_]['selftext'].lower(),\n",
    "                ))\n",
    "                \"\"\"\n",
    "                When we have scraped as many posts from r/suicidewatch as we did from r/depression,\n",
    "                we will end the operation\n",
    "                \"\"\"\n",
    "                if len(data_list) == 2 * dep_count:\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                pass\n",
    "            \"\"\"\n",
    "            In the event of a KeyError, ValueError ,or ChunkedEncodingError due to a break in the data streaming \n",
    "            of the API, the loop will find the last post it processed correctly and restart from there\n",
    "            \"\"\"\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except ValueError:\n",
    "            pass\n",
    "        except ChunkedEncodingError:\n",
    "            pass\n",
    "            \n",
    "    params = {\n",
    "            'subreddit': 'suicidewatch',\n",
    "            'size': 100,\n",
    "            'before': data['data'][-1]['created_utc']\n",
    "            }\n",
    "    time.sleep(5)\n",
    "    print(f\"{len(data_list)} posts gathered.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7edf3da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646235450"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][-1]['created_utc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e574c99",
   "metadata": {},
   "source": [
    "### Saving Scraped r/SuicideWatch Data as a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "abc6ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for suicidewatch posts\n",
    "suicidewatch = pd.DataFrame(data_list[dep_count:], columns=['title', 'selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d469f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "suicidewatch.to_csv('suicidewatch.csv', index_label = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
